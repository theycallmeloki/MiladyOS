apiVersion: batch/v1
kind: Job
metadata:
  name: autodidact-pipeline
  namespace: autodidact
spec:
  template:
    spec:
      containers:
      - name: kubectl
        image: bitnami/kubectl:latest
        command:
        - "bash"
        - "-c"
        - |
          # Wait for a job to complete successfully
          wait_for_job() {
            JOB_NAME=$1
            echo "Waiting for job $JOB_NAME to complete..."
            
            # Loop until job completes
            while true; do
              # Get job status
              STATUS=$(kubectl get job $JOB_NAME -n autodidact -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}')
              FAILED=$(kubectl get job $JOB_NAME -n autodidact -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}')
              
              if [ "$STATUS" == "True" ]; then
                echo "Job $JOB_NAME completed successfully!"
                break
              elif [ "$FAILED" == "True" ]; then
                echo "Job $JOB_NAME failed. Exiting pipeline."
                exit 1
              fi
              
              echo "Job $JOB_NAME still running. Checking again in 30 seconds..."
              sleep 30
            done
          }
          
          # Step 1: Run embeddings job
          echo "Starting embeddings job..."
          kubectl delete job autodidact-embeddings -n autodidact --ignore-not-found
          kubectl apply -f /k8s-configs/autodidact-embeddings-job.yaml
          wait_for_job autodidact-embeddings
          
          # Step 2: Run QA generation job
          echo "Starting QA generation job..."
          kubectl delete job autodidact-qa -n autodidact --ignore-not-found
          kubectl apply -f /k8s-configs/autodidact-qa-job.yaml
          wait_for_job autodidact-qa
          
          # Step 3: Run optimized training job
          echo "Starting training job with GPU optimizations..."
          kubectl delete job autodidact-training -n autodidact --ignore-not-found
          kubectl apply -f /k8s-configs/autodidact-training-job.yaml
          wait_for_job autodidact-training
          
          # Step 4: Verify and log results
          echo "Checking training outputs..."
          kubectl exec -n autodidact data-checker -- ls -la /app/model_output || true
          
          echo "Complete AutoDidact pipeline completed successfully!"
        volumeMounts:
        - name: k8s-configs
          mountPath: /k8s-configs
      volumes:
      - name: k8s-configs
        configMap:
          name: autodidact-k8s-configs
      serviceAccountName: autodidact-pipeline-runner
      restartPolicy: Never
  backoffLimit: 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: autodidact-k8s-configs
  namespace: autodidact
data:
  autodidact-embeddings-job.yaml: |
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: autodidact-embeddings
      namespace: autodidact
    spec:
      template:
        spec:
          runtimeClassName: nvidia
          nodeSelector:
            kubernetes.io/hostname: talos-dkp-mps  # Select the RTX 3090 node
          containers:
          - name: embeddings
            image: ghcr.io/astral-sh/uv:python3.10-bookworm
            command:
            - "bash"
            - "-c"
            - |
              # Set up directories
              mkdir -p /app/data /app/saved_data /app/faiss_index
              
              # Copy the script from the ConfigMap
              cp /embeddings/embeddings.py /app/
              chmod 755 /app/embeddings.py
              
              # Set CUDA_VISIBLE_DEVICES to use the first GPU
              export CUDA_VISIBLE_DEVICES=0
              
              # Run the script
              cd /app
              uv run embeddings.py --input-file /app/data/milady_report.md --chunks-output /app/saved_data/chunks.pkl --faiss-index /app/faiss_index
            resources:
              requests:
                memory: "16Gi"
                cpu: "4"
              limits:
                memory: "32Gi"
                cpu: "8"
            volumeMounts:
            - name: autodidact-pvc
              mountPath: /app/data
              subPath: data
            - name: autodidact-pvc
              mountPath: /app/saved_data
              subPath: saved_data
            - name: autodidact-pvc
              mountPath: /app/faiss_index
              subPath: faiss_index
            - name: autodidact-pvc
              mountPath: /app
            - name: embeddings-script
              mountPath: /embeddings
          volumes:
          - name: autodidact-pvc
            persistentVolumeClaim:
              claimName: autodidact-data
          - name: embeddings-script
            configMap:
              name: autodidact-all-files
          restartPolicy: OnFailure
      backoffLimit: 2
  autodidact-qa-job.yaml: |
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: autodidact-qa
      namespace: autodidact
    spec:
      template:
        spec:
          runtimeClassName: nvidia
          nodeSelector:
            kubernetes.io/hostname: talos-dkp-mps  # Select the RTX 3090 node
          initContainers:
          - name: setup-files
            image: busybox
            command:
            - "/bin/sh"
            - "-c"
            - |
              # Copy necessary files to the app directory
              cp /source-files/generate_data.py /app/
              cp /source-files/embeddings.py /app/
              
              # Ensure proper permissions
              chmod 755 /app/*.py
            volumeMounts:
            - name: autodidact-pvc
              mountPath: /app
            - name: all-files
              mountPath: /source-files
          containers:
          - name: qa-generation
            image: ghcr.io/astral-sh/uv:python3.10-bookworm
            command:
            - "bash"
            - "-c"
            - |
              # Set up directories
              mkdir -p /app/data /app/saved_data /app/faiss_index
              
              # Set CUDA_VISIBLE_DEVICES to use the first GPU
              export CUDA_VISIBLE_DEVICES=0
              
              # Run the script
              cd /app
              uv run generate_data.py --input-file /app/data/milady_report.md --chunks-file /app/saved_data/chunks.pkl --faiss-index /app/faiss_index --output-file /app/saved_data/questions.json
            resources:
              requests:
                memory: "16Gi"
                cpu: "4"
              limits:
                memory: "32Gi"
                cpu: "8"
            volumeMounts:
            - name: autodidact-pvc
              mountPath: /app/data
              subPath: data
            - name: autodidact-pvc
              mountPath: /app/saved_data
              subPath: saved_data
            - name: autodidact-pvc
              mountPath: /app/faiss_index
              subPath: faiss_index
            - name: autodidact-pvc
              mountPath: /app
          volumes:
          - name: autodidact-pvc
            persistentVolumeClaim:
              claimName: autodidact-data
          - name: all-files
            configMap:
              name: autodidact-all-files
          restartPolicy: OnFailure
      backoffLimit: 2
  autodidact-training-job.yaml: |
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: autodidact-training
      namespace: autodidact
    spec:
      template:
        spec:
          runtimeClassName: nvidia
          nodeSelector:
            kubernetes.io/hostname: talos-dkp-mps  # Select the RTX 3090 node
          initContainers:
          - name: setup-files
            image: busybox
            command:
            - "/bin/sh"
            - "-c"
            - |
              # Copy necessary files to the app directory
              cp /source-files/UnslothGRPOTrainerTemp.py /app/
              cp /source-files/rl_helpers.py /app/
              
              # Ensure proper permissions
              chmod 755 /app/*.py
            volumeMounts:
            - name: autodidact-pvc
              mountPath: /app
            - name: all-files
              mountPath: /source-files
          containers:
          - name: training
            image: ghcr.io/astral-sh/uv:python3.10-bookworm
            command:
            - "bash"
            - "-c"
            - |
              # Set up directories
              mkdir -p /app/data /app/saved_data /app/faiss_index /app/grpo_trainer_lora_model
              
              # Set CUDA_VISIBLE_DEVICES to use all available GPUs
              
              # Run the training script
              cd /app
              python -c "
              from unsloth import FastLanguageModel
              import torch
              from rl_helpers import get_qa_dataset, build_reward_correctness_fn, reward_formatting, run_agent
              from UnslothGRPOTrainerTemp import UnslothGRPOConfig, UnslothGRPOTrainer, vLLMSamplingParams
              from vllm import SamplingParams
              
              # Load model
              print('Loading model...')
              max_seq_length = 2048
              lora_rank = 32
              
              model, tokenizer = FastLanguageModel.from_pretrained(
                  model_name = 'meta-llama/meta-Llama-3.1-8B-Instruct',
                  max_seq_length = max_seq_length,
                  load_in_4bit = True,
                  fast_inference = True,
                  max_lora_rank = lora_rank,
                  gpu_memory_utilization = 0.7,
              )
              
              model = FastLanguageModel.get_peft_model(
                  model,
                  r = lora_rank,
                  target_modules = [
                      'q_proj', 'k_proj', 'v_proj', 'o_proj',
                      'gate_proj', 'up_proj', 'down_proj',
                  ],
                  lora_alpha = lora_rank,
                  use_gradient_checkpointing = 'unsloth',
                  random_state = 3407,
              )
              
              # Get dataset
              print('Loading dataset...')
              train_dataset, test_dataset = get_qa_dataset()
              
              # Define agentic generate function
              def agentic_generate(prompts, generate_fn, max_generations=6):
                  return run_agent(generate_fn, tokenizer, prompts, max_generations)
              model.agentic_generate = agentic_generate
              
              # Setup verifier
              print('Setting up reward functions...')
              verifier_sampling_params = SamplingParams(
                  temperature = 0.1,
                  top_p = 0.95,
                  max_tokens = 4096,
              )
              def verifier_generate_fn(inputs):
                  return model.fast_generate(
                      inputs,
                      sampling_params = verifier_sampling_params,
                  )
              
              reward_correctness = build_reward_correctness_fn(verifier_generate_fn, tokenizer)
              
              # Setup training args
              print('Setting up training configuration...')
              training_args = UnslothGRPOConfig(
                  use_vllm = True,
                  use_agentic_generate = True,
                  learning_rate = 5e-6,
                  adam_beta1 = 0.9,
                  adam_beta2 = 0.99,
                  weight_decay = 0.1,
                  warmup_ratio = 0.1,
                  lr_scheduler_type = 'cosine',
                  optim = 'paged_adamw_8bit',
                  logging_steps = 1,
                  bf16 = torch.cuda.is_bf16_supported(),
                  fp16 = not torch.cuda.is_bf16_supported(),
                  per_device_train_batch_size = 8,
                  gradient_accumulation_steps = 1,
                  num_generations = 8,
                  max_prompt_length = 1024,
                  max_completion_length = 1024,
                  max_steps = 101,
                  save_steps = 50,
                  max_grad_norm = 0.1,
                  report_to = 'none',
                  output_dir = 'grpo_trainer_lora_model',
              )
              
              # Setup trainer
              print('Initializing trainer...')
              trainer = UnslothGRPOTrainer(
                  model = model,
                  processing_class = tokenizer,
                  reward_funcs = [
                      reward_correctness,
                      reward_formatting,
                  ],
                  args = training_args,
                  train_dataset = train_dataset,
              )
              
              # Run training
              print('Starting training...')
              trainer.train()
              print('Training complete!')
              "
            resources:
              requests:
                memory: "32Gi"
                cpu: "8"
                nvidia.com/gpu: 1
              limits:
                memory: "64Gi"
                cpu: "16"
                nvidia.com/gpu: 1
            volumeMounts:
            - name: autodidact-pvc
              mountPath: /app/data
              subPath: data
            - name: autodidact-pvc
              mountPath: /app/saved_data
              subPath: saved_data
            - name: autodidact-pvc
              mountPath: /app/faiss_index
              subPath: faiss_index
            - name: autodidact-pvc
              mountPath: /app/grpo_trainer_lora_model
              subPath: grpo_trainer_lora_model
            - name: autodidact-pvc
              mountPath: /app
          volumes:
          - name: autodidact-pvc
            persistentVolumeClaim:
              claimName: autodidact-data
          - name: all-files
            configMap:
              name: autodidact-all-files
          restartPolicy: OnFailure
      backoffLimit: 2
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: autodidact-pipeline-runner
  namespace: autodidact
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: job-manager
  namespace: autodidact
rules:
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pipeline-job-manager
  namespace: autodidact
subjects:
- kind: ServiceAccount
  name: autodidact-pipeline-runner
  namespace: autodidact
roleRef:
  kind: Role
  name: job-manager
  apiGroup: rbac.authorization.k8s.io