apiVersion: v1
kind: ConfigMap
metadata:
  name: autodidact-qa
  namespace: autodidact
data:
  generate_data.py: |
    # /// script
    # requires-python = ">=3.8"
    # dependencies = [
    #   "torch>=2.0.0",
    #   "transformers>=4.49.0",
    #   "langchain",
    #   "langchain-community",
    #   "faiss-cpu",
    #   "unstructured",
    #   "unsloth>=2025.3.6",
    #   "vllm==0.7.2",
    #   "pickle5",
    #   "pydantic",
    #   "setuptools",
    #   "triton",
    # ]
    # ///

    """
    QA generation script for AutoDidact.
    
    This script loads the chunks created by the embeddings step, then generates 
    question-answer pairs using a language model. 
    For each chunk, it generates multiple question-answer pairs with different difficulties.
    The generation is performed in batch for efficiency, with retries for failed prompts.
    Successfully generated QA pairs are saved to "saved_data/questions.json".
    """

    import os
    import re
    import json
    import pickle
    from typing import List, Tuple, Optional, Dict

    def main():
        # Load chunks from pickle file
        chunks_path = "saved_data/chunks.pkl"
        if not os.path.exists(chunks_path):
            print(f"Error: Chunks file not found at {chunks_path}")
            return
            
        with open(chunks_path, "rb") as f:
            chunks = pickle.load(f)
            
        print(f"Loaded {len(chunks)} document chunks")
        
        # Setup model for generation
        from unsloth import FastLanguageModel
        from vllm import SamplingParams
        
        # Load the LLM model (using a smaller model for this demo)
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name="meta-llama/meta-Llama-3.1-8B-Instruct",
            max_seq_length=4096,
            load_in_4bit=True,     # Use 4-bit quantization for memory efficiency
            fast_inference=True,    # Enable fast inference
            gpu_memory_utilization=0.8
        )
        
        # Define sampling parameters for generation
        sampling_params = SamplingParams(
            temperature=0.3,
            top_p=0.95,
            max_tokens=4096,
        )
        
        # Function to format prompts for the model
        def format_input(text: str) -> str:
            return tokenizer.apply_chat_template(
                [{"role": "user", "content": text}],
                tokenize=False,
                add_generation_prompt=True
            )
        
        # Batch generation function
        def batch_generate(prompts: List[str]) -> List[str]:
            formatted = [format_input(p) for p in prompts]
            outputs = model.fast_generate(formatted, sampling_params=sampling_params)
            return [output.outputs[0].text for output in outputs]
        
        # Parse QA blocks from model outputs
        def parse_qa_block(block: str) -> Optional[Tuple[str, str, str]]:
            lines = [line.strip() for line in block.splitlines() if line.strip()]
            if not lines:
                return None
                
            question, answer, difficulty = None, None, None
            for line in lines:
                lower = line.lower()
                if question is None and lower.startswith("question:"):
                    question = line[len("question:"):].strip()
                elif answer is None and lower.startswith("answer:"):
                    answer = line[len("answer:"):].strip()
                elif difficulty is None and lower.startswith("difficulty:"):
                    difficulty = line[len("difficulty:"):].strip()
                    
            if question and answer and difficulty:
                return question, answer, difficulty
            if len(lines) == 3:
                return lines[0], lines[1], lines[2]
            return None
        
        # Parse multiple QA blocks from output
        def parse_multiple_qa_output(output: str) -> List[Tuple[str, str, str]]:
            blocks = re.split(r'\n\s*\n', output.strip())
            qa_pairs = []
            for block in blocks:
                parsed = parse_qa_block(block)
                if parsed:
                    qa_pairs.append(parsed)
            return qa_pairs
        
        # Generate QA pairs for chunks
        def generate_question_batch(chunks, num_questions=2):
            prompts = []
            chunk_ids = []
            
            # Prepare prompts
            for i in range(len(chunks)):
                chunk = chunks[i]
                prompt = (
                    f"From the text provided, generate {num_questions} questions with answers.\n"
                    "For each QA pair, output exactly three lines with no extra commentary:\n"
                    "Line 1: Question: <your question>\n"
                    "Line 2: Answer: <the answer>\n"
                    "Line 3: Difficulty: <easy, medium, or hard>\n"
                    "Do not include any additional text.\n\n"
                    f"Text: {chunk.page_content}\n"
                )
                prompts.append(prompt)
                chunk_ids.append(i)
                
            # First batch generation
            print(f"Generating QA pairs for {len(prompts)} chunks")
            outputs = batch_generate(prompts)
            
            # Parse outputs
            final_questions = []
            for i, output in enumerate(outputs):
                qa_pairs = parse_multiple_qa_output(output)
                if qa_pairs:
                    for qa in qa_pairs:
                        final_questions.append({
                            "chunk_id": chunk_ids[i],
                            "question": qa[0],
                            "answer": qa[1],
                            "difficulty": qa[2]
                        })
                        
            print(f"Generated {len(final_questions)} QA pairs")
            return final_questions
            
        # Generate QA pairs (using a smaller number for this demo)
        all_questions = generate_question_batch(chunks, num_questions=4)
        
        # Save the QA pairs to a JSON file
        os.makedirs("saved_data", exist_ok=True)
        questions_path = os.path.join("saved_data", "questions.json")
        with open(questions_path, "w") as f:
            json.dump(all_questions, f, indent=2)
            
        print(f"Saved questions to {questions_path}")
        
    if __name__ == "__main__":
        main()